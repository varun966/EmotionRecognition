{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e8e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.10.1\n",
      "CUDA Built:  True\n",
      "GPU:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"CUDA Built: \", tf.test.is_built_with_cuda())\n",
    "print(\"GPU: \", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e39a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Growth Set\n"
     ]
    }
   ],
   "source": [
    "# Setting memory growth\n",
    "# By Default, Tensorflow may allocate all GPU memory at once, which can cause issue if \n",
    "# you're running multiple GPU applications\n",
    "# set memory growth tells Tensorflow to only allocate memory as needed, dynamically growing the memory footprint as needed\n",
    "# This helps avoid out-of-memory errors and allows multiple programs to share GPU efficiently/safely\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"Memory Growth Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14882053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as preprocess_mobile\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficient\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67779fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as varun966\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as varun966\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"varun966/EmotionRecognition\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"varun966/EmotionRecognition\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository varun966/EmotionRecognition initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository varun966/EmotionRecognition initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/5463f7ad9066475d87089177ad7424de', creation_time=1752684143899, experiment_id='0', last_update_time=1752684143899, lifecycle_stage='active', name='Mobile Net Experiment', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/varun966/EmotionRecognition.mlflow')\n",
    "dagshub.init(repo_owner='varun966', repo_name='EmotionRecognition', mlflow=True)\n",
    "\n",
    "mlflow.set_experiment(\"Mobile Net Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f6d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e6ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONST\n",
    "img_shape = (224,224,3)\n",
    "drop_layers = -5\n",
    "trainable_layers = 70\n",
    "Epochs = 30\n",
    "Verbose = 1\n",
    "batch_size = 16\n",
    "train_path = r'D:/AIML/fer2013/train'\n",
    "test_path = r'D:/AIML/fer2013/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cb9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before model training, clear Keras session to free old graphs and memory.\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91546a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_path, filename)):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(preprocess_train, filename, file)):\n\u001b[1;32m---> 15\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m         gray \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mcvtColor(img, cv\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)  \u001b[38;5;66;03m# as OpenCv is reading by default in BGR\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         eq_img \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mequalizeHist(gray)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Preprocess the images to equalize the Histogram\n",
    "\n",
    "# preprocess_train =  r'D:/AIML/fer2013/preprocess/train'\n",
    "# preprocess_test = r'D:/AIML/fer2013/preprocess/test'\n",
    "\n",
    "# os.makedirs(preprocess_test, exist_ok=True)\n",
    "# os.makedirs(preprocess_test, exist_ok=True)\n",
    "\n",
    "# for filename in os.listdir(train_path):\n",
    "#     os.makedirs(os.path.join(preprocess_train, filename), exist_ok=True)\n",
    "\n",
    "#     print(filename)\n",
    "#     for file in os.listdir(os.path.join(train_path, filename)):\n",
    "#         if not os.path.exists(os.path.join(preprocess_train, filename, file)):\n",
    "#             img = cv.imread(os.path.join(train_path, filename, file))\n",
    "#             gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)  # as OpenCv is reading by default in BGR\n",
    "\n",
    "#             eq_img = cv.equalizeHist(gray)\n",
    "\n",
    "#             resized_img = cv.resize(eq_img, (224, 224), interpolation=cv.INTER_CUBIC)\n",
    "#             #print(os.path.join(train_path, filename, file))\n",
    "#             blurred = cv.GaussianBlur(resized_img, (3, 3), 0)\n",
    "#             sharpened = cv.addWeighted(blurred, 1.5, blurred, -0.5, 0)\n",
    "\n",
    "\n",
    "#             cv.imwrite(os.path.join(preprocess_train, filename, file), sharpened)\n",
    "\n",
    "\n",
    "# for filename in os.listdir(test_path):\n",
    "#     os.makedirs(os.path.join(preprocess_test, filename), exist_ok=True)\n",
    "\n",
    "#     print(filename)\n",
    "#     for file in os.listdir(os.path.join(test_path, filename)):\n",
    "#         if not os.path.exists(os.path.join(preprocess_test, filename, file)):\n",
    "#             img = cv.imread(os.path.join(test_path, filename, file))\n",
    "#             gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)  # as OpenCv is reading by default in BGR\n",
    "#             eq_img = cv.equalizeHist(gray)\n",
    "\n",
    "#             resized_img = cv.resize(eq_img, (224, 224), interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "#             blurred = cv.GaussianBlur(resized_img, (3, 3), 0)\n",
    "#             sharpened = cv.addWeighted(blurred, 1.5, blurred, -0.5, 0)\n",
    "\n",
    "\n",
    "#             cv.imwrite(os.path.join(preprocess_test, filename, file), sharpened)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95539be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n",
      "Epoch 1/30\n",
      "1436/1436 [==============================] - 392s 271ms/step - loss: 2.6636 - accuracy: 0.1686 - val_loss: 2.4153 - val_accuracy: 0.2871 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "1436/1436 [==============================] - 356s 247ms/step - loss: 2.4619 - accuracy: 0.1955 - val_loss: 2.2875 - val_accuracy: 0.3033 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "1436/1436 [==============================] - 289s 201ms/step - loss: 2.3754 - accuracy: 0.2162 - val_loss: 2.1623 - val_accuracy: 0.3917 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "1436/1436 [==============================] - 277s 193ms/step - loss: 2.2734 - accuracy: 0.2636 - val_loss: 1.9527 - val_accuracy: 0.4005 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "1436/1436 [==============================] - 282s 197ms/step - loss: 2.1634 - accuracy: 0.2968 - val_loss: 1.8038 - val_accuracy: 0.4421 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "1436/1436 [==============================] - 287s 200ms/step - loss: 2.0435 - accuracy: 0.3313 - val_loss: 1.6661 - val_accuracy: 0.4797 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "1436/1436 [==============================] - 291s 202ms/step - loss: 1.9739 - accuracy: 0.3485 - val_loss: 1.5756 - val_accuracy: 0.4959 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "1436/1436 [==============================] - 280s 195ms/step - loss: 1.8932 - accuracy: 0.3530 - val_loss: 1.5256 - val_accuracy: 0.5043 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "1436/1436 [==============================] - 283s 197ms/step - loss: 1.8161 - accuracy: 0.3775 - val_loss: 1.4981 - val_accuracy: 0.4976 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "1436/1436 [==============================] - 290s 202ms/step - loss: 1.7969 - accuracy: 0.3778 - val_loss: 1.4252 - val_accuracy: 0.5196 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "1436/1436 [==============================] - 283s 197ms/step - loss: 1.7559 - accuracy: 0.3900 - val_loss: 1.4480 - val_accuracy: 0.5165 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "1436/1436 [==============================] - 282s 196ms/step - loss: 1.6994 - accuracy: 0.4046 - val_loss: 1.3653 - val_accuracy: 0.5370 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "1436/1436 [==============================] - 350s 243ms/step - loss: 1.6735 - accuracy: 0.4070 - val_loss: 1.3427 - val_accuracy: 0.5414 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "1436/1436 [==============================] - 325s 226ms/step - loss: 1.6598 - accuracy: 0.4091 - val_loss: 1.3629 - val_accuracy: 0.5287 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "1436/1436 [==============================] - 285s 198ms/step - loss: 1.6147 - accuracy: 0.4186 - val_loss: 1.2647 - val_accuracy: 0.5609 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "1436/1436 [==============================] - 280s 195ms/step - loss: 1.5834 - accuracy: 0.4275 - val_loss: 1.2108 - val_accuracy: 0.5839 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "1436/1436 [==============================] - 241s 168ms/step - loss: 1.5664 - accuracy: 0.4347 - val_loss: 1.2331 - val_accuracy: 0.5752 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "1436/1436 [==============================] - 261s 182ms/step - loss: 1.5394 - accuracy: 0.4424 - val_loss: 1.2331 - val_accuracy: 0.5673 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "1436/1436 [==============================] - 280s 195ms/step - loss: 1.5193 - accuracy: 0.4472 - val_loss: 1.2021 - val_accuracy: 0.5788 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "1436/1436 [==============================] - 277s 193ms/step - loss: 1.4911 - accuracy: 0.4526 - val_loss: 1.1703 - val_accuracy: 0.5879 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "1436/1436 [==============================] - 276s 192ms/step - loss: 1.4706 - accuracy: 0.4589 - val_loss: 1.1588 - val_accuracy: 0.5875 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "1436/1436 [==============================] - 280s 195ms/step - loss: 1.4658 - accuracy: 0.4595 - val_loss: 1.1835 - val_accuracy: 0.5823 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "1436/1436 [==============================] - 276s 192ms/step - loss: 1.4327 - accuracy: 0.4714 - val_loss: 1.1715 - val_accuracy: 0.5887 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "1436/1436 [==============================] - 277s 193ms/step - loss: 1.4454 - accuracy: 0.4663 - val_loss: 1.1394 - val_accuracy: 0.5921 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "1436/1436 [==============================] - 279s 194ms/step - loss: 1.4263 - accuracy: 0.4711 - val_loss: 1.1525 - val_accuracy: 0.5830 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "1436/1436 [==============================] - 275s 191ms/step - loss: 1.4221 - accuracy: 0.4733 - val_loss: 1.1129 - val_accuracy: 0.5994 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "1436/1436 [==============================] - 311s 216ms/step - loss: 1.3932 - accuracy: 0.4772 - val_loss: 1.1380 - val_accuracy: 0.5907 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "1436/1436 [==============================] - 297s 206ms/step - loss: 1.3897 - accuracy: 0.4844 - val_loss: 1.0952 - val_accuracy: 0.6032 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "1436/1436 [==============================] - 272s 190ms/step - loss: 1.4026 - accuracy: 0.4766 - val_loss: 1.0904 - val_accuracy: 0.6067 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "1436/1436 [==============================] - 265s 184ms/step - loss: 1.3733 - accuracy: 0.4870 - val_loss: 1.1111 - val_accuracy: 0.6029 - lr: 1.0000e-04\n",
      "Found 7178 images belonging to 7 classes.\n",
      "🏃 View run flawless-flea-823 at: https://dagshub.com/varun966/EmotionRecognition.mlflow/#/experiments/0/runs/4106afa4059041dcbf930e021a5cbb8f\n",
      "🧪 View experiment at: https://dagshub.com/varun966/EmotionRecognition.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------- Start MLflow Run --------------------\n",
    "with mlflow.start_run():\n",
    "    start_time = time.time()\n",
    "\n",
    "    mlflow.log_param(\"Preprocessing\", [\n",
    "        \"Grayscale\",\n",
    "        \"Histogram Equalization\",\n",
    "        \"Resize((224,224), interpolation=cv.INTER_CUBIC) \",\n",
    "        #\"Blurring, (cv.GaussianBlur(resized_img, (3, 3), 0))\",\n",
    "        \"Blurring, (cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)))\",\n",
    "        \"Shapening, (cv.addWeighted(blurred, 1.5, blurred, -0.5, 0))\"\n",
    "\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Creating the model\")\n",
    "\n",
    "        # Load base model\n",
    "        base_model = MobileNet(weights='imagenet', input_shape=img_shape)\n",
    "        model = Sequential()\n",
    "\n",
    "        mlflow.log_param(\"input_shape\", img_shape)\n",
    "        mlflow.log_param(\"pre_loaded_weights\", \"imagenet\")\n",
    "        mlflow.log_param(\"drop_layers\", drop_layers)\n",
    "\n",
    "        for layer in base_model.layers[:drop_layers]:\n",
    "            model.add(layer)\n",
    "\n",
    "        if trainable_layers == 0:\n",
    "            model.trainable = False\n",
    "        elif trainable_layers == 1:\n",
    "            model.trainable = True\n",
    "        elif trainable_layers < 0:\n",
    "            for layer in model.layers[:trainable_layers]:\n",
    "                layer.trainable = False\n",
    "            for layer in model.layers[trainable_layers:]:\n",
    "                layer.trainable = True\n",
    "\n",
    "        mlflow.log_param(\"trainable_layers\", trainable_layers)\n",
    "\n",
    "        trainable_params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
    "        non_trainable_params = np.sum([np.prod(v.get_shape()) for v in model.non_trainable_weights])\n",
    "        total_params = trainable_params + non_trainable_params\n",
    "\n",
    "        mlflow.log_param(\"trainable_params\", int(trainable_params))\n",
    "        mlflow.log_param(\"non_trainable_params\", int(non_trainable_params))\n",
    "        mlflow.log_param(\"total_params\", int(total_params))\n",
    "\n",
    "        # Custom Layers (logged individually)\n",
    "        model.add(GlobalAveragePooling2D(name='global_avg_pool'))\n",
    "        model.add(Dropout(0.5, name='dropout_x'))\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001), name='dense_1'))\n",
    "        model.add(Dropout(0.3, name='dropout_2'))\n",
    "        model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001), name='dense_2'))\n",
    "        model.add(Dropout(0.3, name='dropout_3'))\n",
    "        model.add(Dense(7, activation='softmax', name='output', dtype='float32'))\n",
    "\n",
    "        mlflow.log_param(\"custom_layers\", [\n",
    "            \"GlobalAveragePooling2D\",\n",
    "            \"Dropout(0.5)\",\n",
    "            \"Dense(256, relu, L2=0.001)\",\n",
    "            \"Dropout(0.3)\",\n",
    "            \"Dense(128, relu, L2=0.001)\",\n",
    "            \"Dropout(0.3)\",\n",
    "            \"Dense(7, softmax)\"\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=Adam(learning_rate=1e-4),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # -------------------- Data Augmentation --------------------\n",
    "        # augmentation_params = {\n",
    "        #     \"rotation_range\": 10,\n",
    "        #     \"zoom_range\": 0.1,\n",
    "        #     \"width_shift_range\": 0.1,\n",
    "        #     \"height_shift_range\": 0.1,\n",
    "        #     \"shear_range\": 0.1,\n",
    "        #     \"horizontal_flip\": True,\n",
    "        #     \"fill_mode\": 'nearest'\n",
    "        # }\n",
    "\n",
    "        augmentation_params = {\n",
    "            \"rotation_range\": 10,\n",
    "            \"zoom_range\": [0.1, 1.2],\n",
    "            \"width_shift_range\": 0.1,\n",
    "            \"height_shift_range\": 0.1,\n",
    "            \"shear_range\": 0.1,\n",
    "            \"horizontal_flip\": True,\n",
    "            \"fill_mode\": 'nearest',\n",
    "            \"brightness_range\": [0.8, 1.2],\n",
    "            \"channel_shift_range\": 30.0\n",
    "        }\n",
    "        mlflow.log_params(augmentation_params)\n",
    "\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_mobile,\n",
    "            validation_split=0.2,\n",
    "            **augmentation_params\n",
    "        )\n",
    "\n",
    "        val_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_mobile,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            directory=train_path,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training',\n",
    "            shuffle=True,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            directory=train_path,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation',\n",
    "            shuffle=False,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(train_generator.classes),\n",
    "            y=train_generator.classes)\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        # -------------------- Callbacks --------------------\n",
    "        lr_schedule = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "        # -------------------- Training --------------------\n",
    "        history = model.fit(\n",
    "            x=train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=Epochs,\n",
    "            verbose=Verbose,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=[lr_schedule, early_stop]\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        training_duration = end_time - start_time\n",
    "\n",
    "        # 📝 Log training time as metric in seconds or minutes\n",
    "        mlflow.log_metric(\"training_time_seconds\", training_duration)\n",
    "        mlflow.log_metric(\"training_time_minutes\", training_duration / 60)\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"train_accuracy\": history.history['accuracy'][-1],\n",
    "            \"val_accuracy\": history.history['val_accuracy'][-1],\n",
    "            \"train_loss\": history.history['loss'][-1],\n",
    "            \"val_loss\": history.history['val_loss'][-1]\n",
    "        })\n",
    "\n",
    "        # -------------------- Save Model Summary --------------------\n",
    "        with open(\"model_summary.txt\", \"w\") as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "        # -------------------- Testing --------------------\n",
    "        start_time_test = time.time()\n",
    "        test_batches = ImageDataGenerator(preprocessing_function=preprocess_mobile).flow_from_directory(\n",
    "            directory=test_path,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=10,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        num_test_records = test_batches.n\n",
    "\n",
    "        test_labels = test_batches.classes\n",
    "        predictions = model.predict(x=test_batches, verbose=0)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "        end_time_test = time.time()\n",
    "\n",
    "        total_test_time = (end_time_test - start_time_test)\n",
    "        test_time_per_image = total_test_time / num_test_records\n",
    "        fps = num_test_records / total_test_time\n",
    "\n",
    "\n",
    "        test_accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "        test_f1_weighted = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "        test_f1_macro = f1_score(test_labels, predicted_labels, average='macro')\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"test_f1_weighted\": test_f1_weighted,\n",
    "            \"test_f1_macro\": test_f1_macro,\n",
    "            \"total_test_duration\": total_test_time,\n",
    "            \"test_time_per_image\": test_time_per_image,\n",
    "            \"number_of_test_records\": num_test_records,\n",
    "            \"FPS\": fps\n",
    "\n",
    "        })\n",
    "\n",
    "        # -------------------- Classification Report --------------------\n",
    "        class_report = classification_report(test_labels, predicted_labels, output_dict=False)\n",
    "        with open(\"classification_report.txt\", \"w\") as f:\n",
    "            f.write(class_report)\n",
    "        mlflow.log_artifact(\"classification_report.txt\")\n",
    "\n",
    "        # -------------------- Confusion Matrix --------------------\n",
    "        cm = confusion_matrix(test_labels, predicted_labels)\n",
    "        np.savetxt(\"confusion_matrix.csv\", cm, delimiter=\",\", fmt=\"%d\")\n",
    "        mlflow.log_artifact(\"confusion_matrix.csv\")\n",
    "\n",
    "\n",
    "        # ----------------Model Logging\n",
    "        model.save('best_model.h5')\n",
    "        mlflow.log_artifact('best_model.h5')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        mlflow.log_param(\"error\", str(e))\n",
    "        logging.error(\"Training failed: %s\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce531162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83721500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed046d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
