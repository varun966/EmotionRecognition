{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748b1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Activation, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as preprocess_mobile\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficient\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caf81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion classes\n",
    "emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdbdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the models\n",
    "# Mobile Net\n",
    "#mobile_model_load = load_model('best_model.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29056aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check afterwards\n",
    "\n",
    "# Rebuild same architecture\n",
    "base_model = EfficientNetB0(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "base_model.trainable = True  # Or use .trainable = False if you want to freeze\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "x = Dropout(0.4, name='dropout_x')(x)\n",
    "# x = Dense(256, activation='relu', kernel_regularizer=l2(0.001), name='dense_1')(x)\n",
    "# x = Dropout(0.3, name='dropout_2')(x)\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.001), name='dense_2')(x)\n",
    "x = Dropout(0.3, name='dropout_3')(x)\n",
    "outputs = Dense(7, activation='softmax', name='output', dtype='float32')(x)\n",
    "\n",
    "base_model_loaded = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "# Compile the model again\n",
    "base_model_loaded.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "base_model_loaded.load_weights(\"effnet_model_saved_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8505908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mobile_model_load = load_model(r'D:\\AIML\\EmotionRecognition\\models\\best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f256953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Webcam.....press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# Start Webcam\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Haar Cascade for face detection\n",
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Starting Webcam.....press 'q' to quit.\")\n",
    "\n",
    "pTime = 0\n",
    "overall_emotion = {label: 0 for label in ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']}\n",
    "frame_count = 0\n",
    "inference_interval = 1\n",
    "\n",
    "# To retain previous prediction\n",
    "last_emotion = \"Detecting...\"\n",
    "last_box = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Frame capture failed\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    if frame_count % inference_interval == 0:\n",
    "        # Convert to grayscale and equalize histogram\n",
    "        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        gray = cv.equalizeHist(gray)\n",
    "\n",
    "        # Detect Face\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]  # take only the first face\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img_resized = cv.resize(face_img, (224, 224)).astype('float32')\n",
    "\n",
    "            #face_batch_mobile = np.expand_dims(face_img_resized.copy(), axis=0)\n",
    "            face_batch_eff = np.expand_dims(face_img_resized.copy(), axis=0)\n",
    "\n",
    "            # Preprocessing\n",
    "            #face_batch_mobilenet = preprocess_mobile(face_batch_mobile)\n",
    "            face_batch_effnet = preprocess_efficient(face_batch_eff)\n",
    "\n",
    "            # Predict\n",
    "            #preds_mobile = mobile_model_load.predict(face_batch_mobilenet, verbose=0)\n",
    "            preds_effnet = base_model_loaded.predict(face_batch_effnet, verbose=0)\n",
    "\n",
    "            # Ensemble\n",
    "            #ensemble_pred = (preds_mobile + preds_effnet) / 2.0\n",
    "            #emotion = emotion_labels[np.argmax(preds_mobile)]\n",
    "            emotion = emotion_labels[np.argmax(preds_effnet)]\n",
    "\n",
    "            # Save for next frames\n",
    "            last_emotion = emotion\n",
    "            last_box = (x, y, w, h)\n",
    "\n",
    "            overall_emotion[emotion] += 1\n",
    "\n",
    "    most_common_emotion = max(overall_emotion, key=overall_emotion.get)\n",
    "\n",
    "    # Draw previous prediction if available\n",
    "    if last_box is not None:\n",
    "        x, y, w, h = last_box\n",
    "        cv.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv.putText(frame, last_emotion, (x, y-10), cv.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "        cv.putText(frame, f'Overall: {most_common_emotion}', (x+100, y+h+20), cv.FONT_HERSHEY_PLAIN, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    # Always calculate FPS\n",
    "    cTime = time.time()\n",
    "    fps = 1 / (cTime - pTime)\n",
    "    pTime = cTime\n",
    "    \n",
    "    # Always show FPS\n",
    "    cv.putText(frame, f'FPS: {int(fps)}', (10, 30), cv.FONT_HERSHEY_PLAIN, 1, (36, 255, 12), 2)\n",
    "\n",
    "    frame_count += 1\n",
    "    cv.imshow('RealTime Emotion Detection', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e7792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
