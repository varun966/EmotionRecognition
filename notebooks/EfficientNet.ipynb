{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e139819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.10.1\n",
      "CUDA Built:  True\n",
      "GPU:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"CUDA Built: \", tf.test.is_built_with_cuda())\n",
    "print(\"GPU: \", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2110884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Growth Set\n"
     ]
    }
   ],
   "source": [
    "# Setting memory growth\n",
    "# By Default, Tensorflow may allocate all GPU memory at once, which can cause issue if \n",
    "# you're running multiple GPU applications\n",
    "# set memory growth tells Tensorflow to only allocate memory as needed, dynamically growing the memory footprint as needed\n",
    "# This helps avoid out-of-memory errors and allows multiple programs to share GPU efficiently/safely\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"Memory Growth Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b642e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Activation, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as preprocess_mobile\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficient\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354c2b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as varun966\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as varun966\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"varun966/EmotionRecognition\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"varun966/EmotionRecognition\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository varun966/EmotionRecognition initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository varun966/EmotionRecognition initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/c2681ea7d2494a459bebc5b988ddf649', creation_time=1753187636321, experiment_id='1', last_update_time=1753187636321, lifecycle_stage='active', name='Efficient Net Experiment', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/varun966/EmotionRecognition.mlflow')\n",
    "dagshub.init(repo_owner='varun966', repo_name='EmotionRecognition', mlflow=True)\n",
    "\n",
    "mlflow.set_experiment(\"Efficient Net Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b03d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cea079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONST\n",
    "img_shape = (224,224,3)\n",
    "drop_layers = -5\n",
    "trainable_layers = -1\n",
    "Epochs = 50\n",
    "Verbose = 1\n",
    "batch_size = 8\n",
    "train_path = r'D:/AIML/fer2013/train'\n",
    "test_path = r'D:/AIML/fer2013/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd95ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before model training, clear Keras session to free old graphs and memory.\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09379c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n",
      "disgust\n",
      "fear\n",
      "happy\n",
      "neutral\n",
      "sad\n",
      "surprise\n",
      "angry\n",
      "disgust\n",
      "fear\n",
      "happy\n",
      "neutral\n",
      "sad\n",
      "surprise\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the images to equalize the Histogram\n",
    "\n",
    "preprocess_train =  r'D:/AIML/fer2013/preprocess/train'\n",
    "preprocess_test = r'D:/AIML/fer2013/preprocess/test'\n",
    "\n",
    "os.makedirs(preprocess_test, exist_ok=True)\n",
    "os.makedirs(preprocess_test, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(train_path):\n",
    "    os.makedirs(os.path.join(preprocess_train, filename), exist_ok=True)\n",
    "\n",
    "    print(filename)\n",
    "    for file in os.listdir(os.path.join(train_path, filename)):\n",
    "        if not os.path.exists(os.path.join(preprocess_train, filename, file)):\n",
    "            img = cv.imread(os.path.join(train_path, filename, file))\n",
    "            gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)  # as OpenCv is reading by default in BGR\n",
    "\n",
    "            eq_img = cv.equalizeHist(gray)\n",
    "\n",
    "\n",
    "            cv.imwrite(os.path.join(preprocess_train, filename, file), eq_img)\n",
    "\n",
    "\n",
    "for filename in os.listdir(test_path):\n",
    "    os.makedirs(os.path.join(preprocess_test, filename), exist_ok=True)\n",
    "\n",
    "    print(filename)\n",
    "    for file in os.listdir(os.path.join(test_path, filename)):\n",
    "        if not os.path.exists(os.path.join(preprocess_test, filename, file)):\n",
    "            img = cv.imread(os.path.join(test_path, filename, file))\n",
    "            gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)  # as OpenCv is reading by default in BGR\n",
    "            eq_img = cv.equalizeHist(gray)\n",
    "\n",
    "\n",
    "\n",
    "            cv.imwrite(os.path.join(preprocess_test, filename, file), eq_img)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fabec076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n",
      "Epoch 1/50\n",
      "2871/2871 [==============================] - 553s 190ms/step - loss: 2.1611 - accuracy: 0.1833 - val_loss: 1.9372 - val_accuracy: 0.3400 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "2871/2871 [==============================] - 553s 193ms/step - loss: 2.0025 - accuracy: 0.2761 - val_loss: 1.6438 - val_accuracy: 0.4550 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "2871/2871 [==============================] - 595s 207ms/step - loss: 1.8695 - accuracy: 0.3336 - val_loss: 1.4557 - val_accuracy: 0.5104 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "2871/2871 [==============================] - 575s 200ms/step - loss: 1.7910 - accuracy: 0.3683 - val_loss: 1.4040 - val_accuracy: 0.5245 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "2871/2871 [==============================] - 589s 205ms/step - loss: 1.7199 - accuracy: 0.3817 - val_loss: 1.3699 - val_accuracy: 0.5267 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "2871/2871 [==============================] - 542s 189ms/step - loss: 1.6686 - accuracy: 0.3915 - val_loss: 1.2951 - val_accuracy: 0.5565 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "2871/2871 [==============================] - 535s 186ms/step - loss: 1.6290 - accuracy: 0.4100 - val_loss: 1.2506 - val_accuracy: 0.5576 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "2871/2871 [==============================] - 556s 194ms/step - loss: 1.5851 - accuracy: 0.4188 - val_loss: 1.2487 - val_accuracy: 0.5626 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "2871/2871 [==============================] - 570s 198ms/step - loss: 1.5512 - accuracy: 0.4288 - val_loss: 1.1829 - val_accuracy: 0.5914 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "2871/2871 [==============================] - 587s 205ms/step - loss: 1.5121 - accuracy: 0.4410 - val_loss: 1.2465 - val_accuracy: 0.5659 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "2871/2871 [==============================] - 566s 197ms/step - loss: 1.5064 - accuracy: 0.4438 - val_loss: 1.1758 - val_accuracy: 0.5750 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "2871/2871 [==============================] - 578s 201ms/step - loss: 1.4895 - accuracy: 0.4444 - val_loss: 1.1178 - val_accuracy: 0.6048 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "2871/2871 [==============================] - 566s 197ms/step - loss: 1.4547 - accuracy: 0.4579 - val_loss: 1.1297 - val_accuracy: 0.5966 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "2871/2871 [==============================] - 485s 169ms/step - loss: 1.4335 - accuracy: 0.4574 - val_loss: 1.1361 - val_accuracy: 0.5922 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "2871/2871 [==============================] - ETA: 0s - loss: 1.4226 - accuracy: 0.4586\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "2871/2871 [==============================] - 423s 147ms/step - loss: 1.4226 - accuracy: 0.4586 - val_loss: 1.1880 - val_accuracy: 0.5898 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "2871/2871 [==============================] - 412s 143ms/step - loss: 1.3777 - accuracy: 0.4796 - val_loss: 1.0838 - val_accuracy: 0.6161 - lr: 5.0000e-05\n",
      "Epoch 17/50\n",
      "2871/2871 [==============================] - 420s 146ms/step - loss: 1.3607 - accuracy: 0.4826 - val_loss: 1.0919 - val_accuracy: 0.6150 - lr: 5.0000e-05\n",
      "Epoch 18/50\n",
      "2871/2871 [==============================] - 413s 144ms/step - loss: 1.3569 - accuracy: 0.4870 - val_loss: 1.0973 - val_accuracy: 0.6076 - lr: 5.0000e-05\n",
      "Epoch 19/50\n",
      "2871/2871 [==============================] - 437s 152ms/step - loss: 1.3306 - accuracy: 0.4929 - val_loss: 1.0793 - val_accuracy: 0.6177 - lr: 5.0000e-05\n",
      "Epoch 20/50\n",
      "2871/2871 [==============================] - 445s 155ms/step - loss: 1.3151 - accuracy: 0.4969 - val_loss: 1.0796 - val_accuracy: 0.6215 - lr: 5.0000e-05\n",
      "Epoch 21/50\n",
      "2871/2871 [==============================] - 547s 190ms/step - loss: 1.3046 - accuracy: 0.5023 - val_loss: 1.1074 - val_accuracy: 0.6170 - lr: 5.0000e-05\n",
      "Epoch 22/50\n",
      "2871/2871 [==============================] - 1349s 470ms/step - loss: 1.3038 - accuracy: 0.5011 - val_loss: 1.0591 - val_accuracy: 0.6269 - lr: 5.0000e-05\n",
      "Epoch 23/50\n",
      "2871/2871 [==============================] - 429s 149ms/step - loss: 1.2817 - accuracy: 0.5101 - val_loss: 1.0721 - val_accuracy: 0.6198 - lr: 5.0000e-05\n",
      "Epoch 24/50\n",
      "2871/2871 [==============================] - 457s 159ms/step - loss: 1.2866 - accuracy: 0.5033 - val_loss: 1.0753 - val_accuracy: 0.6225 - lr: 5.0000e-05\n",
      "Epoch 25/50\n",
      "2871/2871 [==============================] - ETA: 0s - loss: 1.2667 - accuracy: 0.5113\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "2871/2871 [==============================] - 440s 153ms/step - loss: 1.2667 - accuracy: 0.5113 - val_loss: 1.0948 - val_accuracy: 0.6217 - lr: 5.0000e-05\n",
      "Epoch 26/50\n",
      "2871/2871 [==============================] - 474s 165ms/step - loss: 1.2495 - accuracy: 0.5183 - val_loss: 1.0663 - val_accuracy: 0.6250 - lr: 2.5000e-05\n",
      "Epoch 27/50\n",
      "2871/2871 [==============================] - 631s 220ms/step - loss: 1.2205 - accuracy: 0.5250 - val_loss: 1.0826 - val_accuracy: 0.6224 - lr: 2.5000e-05\n",
      "Epoch 28/50\n",
      "2871/2871 [==============================] - ETA: 0s - loss: 1.2181 - accuracy: 0.5208\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "2871/2871 [==============================] - 687s 239ms/step - loss: 1.2181 - accuracy: 0.5208 - val_loss: 1.0741 - val_accuracy: 0.6269 - lr: 2.5000e-05\n",
      "Epoch 29/50\n",
      "2871/2871 [==============================] - 554s 193ms/step - loss: 1.2334 - accuracy: 0.5288 - val_loss: 1.0796 - val_accuracy: 0.6238 - lr: 1.2500e-05\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Training failed: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run aged-owl-95 at: https://dagshub.com/varun966/EmotionRecognition.mlflow/#/experiments/1/runs/3d86dc5895ce4d35bc3d7b2b2ea97034\n",
      "üß™ View experiment at: https://dagshub.com/varun966/EmotionRecognition.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "preprocess_train =  r'D:/AIML/fer2013/preprocess/train'\n",
    "preprocess_test = r'D:/AIML/fer2013/preprocess/test'\n",
    "\n",
    "\n",
    "# -------------------- Start MLflow Run --------------------\n",
    "with mlflow.start_run():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # mlflow.log_param(\"Preprocessing\", [\n",
    "    #     \"Grayscale\",\n",
    "    #     \"Histogram Equalization\",\n",
    "    #     \"Resize((224,224), interpolation=cv.INTER_CUBIC) \",\n",
    "    #     #\"Blurring, (cv.GaussianBlur(resized_img, (3, 3), 0))\",\n",
    "    #     \"Blurring, (cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)))\",\n",
    "    #     \"Shapening, (cv.addWeighted(blurred, 1.5, blurred, -0.5, 0))\"\n",
    "\n",
    "    # ])\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Creating the model\")\n",
    "\n",
    "        # Load EfficientNetB0 as base model\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=img_shape)\n",
    "        base_model.trainable = True\n",
    "        mlflow.log_param(\"input_shape\", img_shape)\n",
    "        mlflow.log_param(\"pre_loaded_weights\", \"imagenet\")\n",
    "        mlflow.log_param(\"drop_layers\", drop_layers)\n",
    "\n",
    "        # Freeze all layers initially\n",
    "        # for layer in base_model.layers:\n",
    "        #     layer.trainable = True\n",
    "\n",
    "        # # Unfreeze last `trainable_layers` if specified\n",
    "        # if trainable_layers > 0:\n",
    "        #     for layer in base_model.layers[-trainable_layers:]:\n",
    "        #         layer.trainable = True\n",
    "        # elif trainable_layers == -1:\n",
    "        #     for layer in base_model.layers:\n",
    "        #         layer.trainable = True\n",
    "\n",
    "        mlflow.log_param(\"trainable_layers\", trainable_layers)\n",
    "\n",
    "        # Build full model with custom head\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "        x = Dropout(0.4, name='dropout_x')(x)\n",
    "        # x = Dense(256, activation='relu', kernel_regularizer=l2(0.001), name='dense_1')(x)\n",
    "        # x = Dropout(0.3, name='dropout_2')(x)\n",
    "        x = Dense(128, activation='relu', kernel_regularizer=l2(0.001), name='dense_2')(x)\n",
    "        x = Dropout(0.3, name='dropout_3')(x)\n",
    "        outputs = Dense(7, activation='softmax', name='output', dtype='float32')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "        trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "        non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])\n",
    "        total_params = trainable_params + non_trainable_params\n",
    "\n",
    "        mlflow.log_param(\"trainable_params\", int(trainable_params))\n",
    "        mlflow.log_param(\"non_trainable_params\", int(non_trainable_params))\n",
    "        mlflow.log_param(\"total_params\", int(total_params))\n",
    "\n",
    "        mlflow.log_param(\"custom_layers\", [\n",
    "            \"GlobalAveragePooling2D\",\n",
    "            \"Dropout(0.4)\",\n",
    "            # \"Dense(256, relu, L2=0.001)\",\n",
    "            # \"Dropout(0.3)\",\n",
    "            \"Dense(128, relu, L2=0.001)\",\n",
    "            \"Dropout(0.3)\",\n",
    "            \"Dense(7, softmax)\"\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=Adam(learning_rate=1e-4),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # -------------------- Data Augmentation --------------------\n",
    "        # augmentation_params = {\n",
    "        #     \"rotation_range\": 10,\n",
    "        #     \"zoom_range\": 0.1,\n",
    "        #     \"width_shift_range\": 0.1,\n",
    "        #     \"height_shift_range\": 0.1,\n",
    "        #     \"shear_range\": 0.1,\n",
    "        #     \"horizontal_flip\": True,\n",
    "        #     \"fill_mode\": 'nearest'\n",
    "        # }\n",
    "\n",
    "        augmentation_params = {\n",
    "            \"rotation_range\": 10,\n",
    "            \"zoom_range\": [0.1, 1.2],\n",
    "            \"width_shift_range\": 0.1,\n",
    "            \"height_shift_range\": 0.1,\n",
    "            \"shear_range\": 0.1,\n",
    "            \"horizontal_flip\": True,\n",
    "            \"fill_mode\": 'nearest',\n",
    "            \"brightness_range\": [0.8, 1.2],\n",
    "            \"channel_shift_range\": 30.0\n",
    "        }\n",
    "\n",
    "        # augmentation_params = {\n",
    "        # \"rotation_range\" :  15,\n",
    "        # \"zoom_range\" : 0.15,\n",
    "        # \"width_shift_range\": 0.1,\n",
    "        # \"height_shift_range\": 0.1,\n",
    "        # \"shear_range\" : 0.1,\n",
    "        # \"horizontal_flip\" : True,\n",
    "        # \"fill_mode\" : 'nearest',\n",
    "\n",
    "        # }\n",
    "        mlflow.log_params(augmentation_params)\n",
    "\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_efficient,\n",
    "            validation_split=0.2,\n",
    "            **augmentation_params\n",
    "        )\n",
    "\n",
    "        val_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_efficient,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            directory=preprocess_train,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training',\n",
    "            shuffle=True,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            directory=preprocess_train,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation',\n",
    "            shuffle=False,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(train_generator.classes),\n",
    "            y=train_generator.classes)\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        # -------------------- Callbacks --------------------\n",
    "        lr_schedule = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "        # -------------------- Training --------------------\n",
    "        history = model.fit(\n",
    "            x=train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=Epochs,\n",
    "            verbose=Verbose,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=[lr_schedule, early_stop]\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        training_duration = end_time - start_time\n",
    "\n",
    "        # üìù Log training time as metric in seconds or minutes\n",
    "        mlflow.log_metric(\"training_time_seconds\", training_duration)\n",
    "        mlflow.log_metric(\"training_time_minutes\", training_duration / 60)\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"train_accuracy\": history.history['accuracy'][-1],\n",
    "            \"val_accuracy\": history.history['val_accuracy'][-1],\n",
    "            \"train_loss\": history.history['loss'][-1],\n",
    "            \"val_loss\": history.history['val_loss'][-1]\n",
    "        })\n",
    "\n",
    "        # -------------------- Save Model Summary --------------------\n",
    "        with open(\"model_summary.txt\", \"w\") as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "        # -------------------- Testing --------------------\n",
    "        start_time_test = time.time()\n",
    "        test_batches = ImageDataGenerator(preprocessing_function=preprocess_efficient).flow_from_directory(\n",
    "            directory=preprocess_test,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=10,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        num_test_records = test_batches.n\n",
    "\n",
    "        test_labels = test_batches.classes\n",
    "        predictions = model.predict(x=test_batches, verbose=0)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "        end_time_test = time.time()\n",
    "\n",
    "        total_test_time = (end_time_test - start_time_test)\n",
    "        test_time_per_image = total_test_time / num_test_records\n",
    "        fps = num_test_records / total_test_time\n",
    "\n",
    "\n",
    "        test_accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "        test_f1_weighted = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "        test_f1_macro = f1_score(test_labels, predicted_labels, average='macro')\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"test_f1_weighted\": test_f1_weighted,\n",
    "            \"test_f1_macro\": test_f1_macro,\n",
    "            \"total_test_duration\": total_test_time,\n",
    "            \"test_time_per_image\": test_time_per_image,\n",
    "            \"number_of_test_records\": num_test_records,\n",
    "            \"FPS\": fps\n",
    "\n",
    "        })\n",
    "\n",
    "        # -------------------- Classification Report --------------------\n",
    "        class_report = classification_report(test_labels, predicted_labels, output_dict=False)\n",
    "        with open(\"classification_report.txt\", \"w\") as f:\n",
    "            f.write(class_report)\n",
    "        mlflow.log_artifact(\"classification_report.txt\")\n",
    "\n",
    "        # -------------------- Confusion Matrix --------------------\n",
    "        cm = confusion_matrix(test_labels, predicted_labels)\n",
    "        np.savetxt(\"confusion_matrix.csv\", cm, delimiter=\",\", fmt=\"%d\")\n",
    "        mlflow.log_artifact(\"confusion_matrix.csv\")\n",
    "\n",
    "\n",
    "        # ----------------Model Logging\n",
    "        model.save('effnet_best_model.h5')\n",
    "        model.save_weights(\"effnet_model_saved_weights.h5\")\n",
    "        mlflow.log_artifact('effnet_best_model.h5')\n",
    "        mlflow.log_artifact('effnet_model_saved_weights.h5')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        mlflow.log_param(\"error\", str(e))\n",
    "        logging.error(\"Training failed: %s\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4a866e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"effnet_model_saved_weights.h5\")\n",
    "\n",
    "# Ran up until epoch 24, saved the model\n",
    "# re load the weights and run the model again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb166e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
