{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "252ae62b-1416-480a-8143-63f9751ceeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.10.1\n",
      "CUDA Built:  True\n",
      "GPU:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"CUDA Built: \", tf.test.is_built_with_cuda())\n",
    "print(\"GPU: \", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4e18f1-050a-400f-9f02-4579bd15c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting memory growth\n",
    "# By Default, Tensorflow may allocate all GPU memory at once, which can cause issue if \n",
    "# you're running multiple GPU applications\n",
    "# set memory growth tells Tensorflow to only allocate memory as needed, dynamically growing the memory footprint as needed\n",
    "# This helps avoid out-of-memory errors and allows multiple programs to share GPU efficiently/safely\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a3f9a9-ab2d-43aa-8c4a-bbe48280d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting mixed precision\n",
    "\n",
    "# Mixed precision uses both float16 and float32 during training\n",
    "# uses float16 where possible \n",
    "# keeps critical variables in float32 for numerical stability\n",
    "\n",
    "#tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98f830e-4f56-4cb7-a5fb-15a63cab1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as preprocess_mobile\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficient\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912ff41-811a-4f46-9063-ff75de39e681",
   "metadata": {},
   "source": [
    "### MobileNet V1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30516d50-3e88-4542-a5d1-09021e20537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet Model\n",
    "\n",
    "mobile = MobileNet(weights='imagenet', input_shape=(224,224,3)) \n",
    "# mobile = MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3697999-a2b6-4dda-827f-d38878767add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Mobil Net Model\n",
    "mobile_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb56988a-d283-4612-bab9-5b0d99ae41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for layer in mobile.layers[:-5]:\n",
    "    mobile_model.add(layer)\n",
    "    c +=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7aa647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3206976\n",
      "21888\n",
      "3228864\n"
     ]
    }
   ],
   "source": [
    "trainable_params = np.sum([np.prod(v.get_shape()) for v in mobile_model.trainable_weights])\n",
    "non_trainable_params = np.sum([np.prod(v.get_shape()) for v in mobile_model.non_trainable_weights])\n",
    "total_params = trainable_params + non_trainable_params\n",
    "    \n",
    "print(trainable_params)\n",
    "print(non_trainable_params)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6d8e6f-0c55-4979-b4f7-8df4632bbda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv2D)              (None, 112, 112, 32)      864       \n",
      "                                                                 \n",
      " conv1_bn (BatchNormalizatio  (None, 112, 112, 32)     128       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv1_relu (ReLU)           (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)     288       \n",
      "                                                                 \n",
      " conv_dw_1_bn (BatchNormaliz  (None, 112, 112, 32)     128       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_1_relu (ReLU)       (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv_pw_1 (Conv2D)          (None, 112, 112, 64)      2048      \n",
      "                                                                 \n",
      " conv_pw_1_bn (BatchNormaliz  (None, 112, 112, 64)     256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_1_relu (ReLU)       (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv_pad_2 (ZeroPadding2D)  (None, 113, 113, 64)      0         \n",
      "                                                                 \n",
      " conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)       576       \n",
      "                                                                 \n",
      " conv_dw_2_bn (BatchNormaliz  (None, 56, 56, 64)       256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_2_relu (ReLU)       (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv_pw_2 (Conv2D)          (None, 56, 56, 128)       8192      \n",
      "                                                                 \n",
      " conv_pw_2_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_2_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)      1152      \n",
      "                                                                 \n",
      " conv_dw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_3 (Conv2D)          (None, 56, 56, 128)       16384     \n",
      "                                                                 \n",
      " conv_pw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_pad_4 (ZeroPadding2D)  (None, 57, 57, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)      1152      \n",
      "                                                                 \n",
      " conv_dw_4_bn (BatchNormaliz  (None, 28, 28, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_4_relu (ReLU)       (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_4 (Conv2D)          (None, 28, 28, 256)       32768     \n",
      "                                                                 \n",
      " conv_pw_4_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_4_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)      2304      \n",
      "                                                                 \n",
      " conv_dw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_5 (Conv2D)          (None, 28, 28, 256)       65536     \n",
      "                                                                 \n",
      " conv_pw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_pad_6 (ZeroPadding2D)  (None, 29, 29, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)      2304      \n",
      "                                                                 \n",
      " conv_dw_6_bn (BatchNormaliz  (None, 14, 14, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_6_relu (ReLU)       (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_6 (Conv2D)          (None, 14, 14, 512)       131072    \n",
      "                                                                 \n",
      " conv_pw_6_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_6_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_7 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_8 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_9 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_10 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_10 (Conv2D)         (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_11 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_11 (Conv2D)         (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)      0         \n",
      "                                                                 \n",
      " conv_dw_12 (DepthwiseConv2D  (None, 7, 7, 512)        4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_12_bn (BatchNormali  (None, 7, 7, 512)        2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        524288    \n",
      "                                                                 \n",
      " conv_pw_12_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_dw_13 (DepthwiseConv2D  (None, 7, 7, 1024)       9216      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1048576   \n",
      "                                                                 \n",
      " conv_pw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobile_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81bdb0a7-b6c1-497f-a62d-78316869949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_layers = 50 # To be set as a constant\n",
    "\n",
    "if trainable_layers == 0: \n",
    "    mobile_model.trainable = False\n",
    "elif trainable_layers == 1:\n",
    "    mobile_model.trainable = True\n",
    "elif trainable_layers < 0:\n",
    "    for layer in mobile_model.layers[:trainable_layers]:\n",
    "        layer.trainable = False\n",
    "    for layer in mobile_model.layers[trainable_layers:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "# for layer in mobile_model.layers[:-50]:\n",
    "#     layer.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba65c73f-7ec4-42b6-841b-c94f8afb5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_model.add(GlobalAveragePooling2D())\n",
    "mobile_model.add(Dropout(0.5, name='dropout_x'))\n",
    "mobile_model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001), name='dense_1'))\n",
    "mobile_model.add(Dropout(0.3, name='dropout_2'))\n",
    "# mobile_model.add(Dense(7, activation='softmax', name='output'))\n",
    "mobile_model.add(Dense(7, activation='softmax', name='output', dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca957789-7de5-4823-b90d-415874d17d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e22aac65-fa15-4a59-8ff4-fd73a9d698f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'D:/AIML/fer2013/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "467046fb-fd43-4c8d-9a58-81074ca2d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Apply data augmentation and MobileNet preprocessing to train the data\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_mobile,\n",
    "    rotation_range = 10,\n",
    "    zoom_range = 0.1,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0.1,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = 'nearest',\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_mobile,\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = train_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory = train_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation',\n",
    "    shuffle = False,      # Turn shuffle off for validation\n",
    "    seed = 42\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e0276f-7f60-4fcf-9965-48bd1a384e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Class weights\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight = 'balanced',\n",
    "    classes = np.unique(train_generator.classes),\n",
    "    y = train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ee3718c-ba4a-444d-b36a-0ebcf12664f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.02664044, 9.40155546, 1.00095877, 0.56845857, 0.82606819,\n",
       "       0.84915705, 1.29331607])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98215a2d-6d38-46a7-b3bb-777416be4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f176bcab-a50c-497a-b846-47d6a2592602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0266404434114071,\n",
       " 1: 9.401555464592715,\n",
       " 2: 1.0009587727708533,\n",
       " 3: 0.5684585684585685,\n",
       " 4: 0.826068191627104,\n",
       " 5: 0.8491570541259982,\n",
       " 6: 1.2933160650937552}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9028a39f-049b-4f05-aada-8c5381706fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring callbacks\n",
    "\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', patience=7, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b90c0a4c-c3de-42f9-83be-2ec6869810c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "718/718 [==============================] - 190s 261ms/step - loss: 2.1269 - accuracy: 0.2847 - val_loss: 1.6938 - val_accuracy: 0.4639 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "718/718 [==============================] - 183s 255ms/step - loss: 1.7124 - accuracy: 0.4247 - val_loss: 1.5189 - val_accuracy: 0.5097 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "718/718 [==============================] - 186s 259ms/step - loss: 1.5366 - accuracy: 0.4893 - val_loss: 1.4219 - val_accuracy: 0.5501 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "718/718 [==============================] - 205s 286ms/step - loss: 1.4131 - accuracy: 0.5361 - val_loss: 1.3635 - val_accuracy: 0.5677 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "718/718 [==============================] - 211s 294ms/step - loss: 1.3187 - accuracy: 0.5603 - val_loss: 1.2553 - val_accuracy: 0.6060 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "718/718 [==============================] - 188s 262ms/step - loss: 1.2334 - accuracy: 0.5913 - val_loss: 1.2994 - val_accuracy: 0.5931 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "718/718 [==============================] - 186s 260ms/step - loss: 1.1730 - accuracy: 0.6009 - val_loss: 1.1990 - val_accuracy: 0.6213 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "718/718 [==============================] - 185s 257ms/step - loss: 1.1037 - accuracy: 0.6223 - val_loss: 1.1565 - val_accuracy: 0.6300 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "718/718 [==============================] - 181s 252ms/step - loss: 1.0701 - accuracy: 0.6388 - val_loss: 1.1769 - val_accuracy: 0.6276 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "718/718 [==============================] - 181s 251ms/step - loss: 1.0139 - accuracy: 0.6488 - val_loss: 1.1530 - val_accuracy: 0.6283 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "718/718 [==============================] - 183s 254ms/step - loss: 0.9669 - accuracy: 0.6623 - val_loss: 1.1381 - val_accuracy: 0.6248 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "718/718 [==============================] - 181s 252ms/step - loss: 0.9205 - accuracy: 0.6783 - val_loss: 1.1138 - val_accuracy: 0.6492 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "718/718 [==============================] - 184s 256ms/step - loss: 0.8920 - accuracy: 0.6920 - val_loss: 1.1523 - val_accuracy: 0.6255 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "718/718 [==============================] - 186s 259ms/step - loss: 0.8693 - accuracy: 0.6965 - val_loss: 1.1849 - val_accuracy: 0.6269 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.8355 - accuracy: 0.7112\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "718/718 [==============================] - 186s 259ms/step - loss: 0.8355 - accuracy: 0.7112 - val_loss: 1.1483 - val_accuracy: 0.6306 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "718/718 [==============================] - 187s 261ms/step - loss: 0.7703 - accuracy: 0.7296 - val_loss: 1.0844 - val_accuracy: 0.6598 - lr: 5.0000e-05\n",
      "Epoch 17/30\n",
      "718/718 [==============================] - 187s 260ms/step - loss: 0.7284 - accuracy: 0.7480 - val_loss: 1.1216 - val_accuracy: 0.6499 - lr: 5.0000e-05\n",
      "Epoch 18/30\n",
      "718/718 [==============================] - 471s 657ms/step - loss: 0.6954 - accuracy: 0.7564 - val_loss: 1.1251 - val_accuracy: 0.6619 - lr: 5.0000e-05\n",
      "Epoch 19/30\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.7632\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "718/718 [==============================] - 662s 922ms/step - loss: 0.6782 - accuracy: 0.7632 - val_loss: 1.1296 - val_accuracy: 0.6617 - lr: 5.0000e-05\n",
      "Epoch 20/30\n",
      "718/718 [==============================] - 215s 299ms/step - loss: 0.6334 - accuracy: 0.7821 - val_loss: 1.1223 - val_accuracy: 0.6631 - lr: 2.5000e-05\n",
      "Epoch 21/30\n",
      "718/718 [==============================] - 188s 262ms/step - loss: 0.6161 - accuracy: 0.7865 - val_loss: 1.1494 - val_accuracy: 0.6602 - lr: 2.5000e-05\n",
      "Epoch 22/30\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.6007 - accuracy: 0.7935\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "718/718 [==============================] - 190s 264ms/step - loss: 0.6007 - accuracy: 0.7935 - val_loss: 1.1663 - val_accuracy: 0.6640 - lr: 2.5000e-05\n",
      "Epoch 23/30\n",
      "718/718 [==============================] - 191s 266ms/step - loss: 0.5742 - accuracy: 0.8009 - val_loss: 1.1668 - val_accuracy: 0.6675 - lr: 1.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d6eaba130>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training \n",
    "Epochs = 30\n",
    "Verbose = 1\n",
    "\n",
    "mobile_model.fit( x = train_generator,\n",
    "                 validation_data = val_generator,\n",
    "                 epochs = Epochs,\n",
    "                 verbose = Verbose,\n",
    "                 class_weight = class_weights,\n",
    "                 callbacks = [lr_schedule, early_stop]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05868405-6faf-442c-96a2-40ebc2c264c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbd3d148-3317-4610-9d49-337c7cc41d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_model.save('mobile_model_saved.keras')  # Keras native format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92a8c7e-87be-48e6-b5d7-af2f0d20d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "mobile_model_load = load_model('mobile_model_saved.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "206574aa-78a0-46d4-820f-217c45366334",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r'D:/AIML/fer2013/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3726588-1dd2-4a26-adc6-1e8c0bbb2b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_mobile).flow_from_directory(\n",
    "    directory = test_path,\n",
    "    target_size=(224,224),\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa5ddfda-cc9a-4895-97d9-a8eaafadbf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c97328c6-2ccb-4c5b-bb65-6ccee02704cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mobile_model_load.predict(x=test_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19f5893d-6d15-4de7-a4f0-a0fe6b010d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c74090a-2538-4266-9049-85ae7912547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Average Type       | Meaning                                                                                                                              | Use When                                                                      |\n",
    "# | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------- |\n",
    "# | `'macro'`          | Compute F1 for each class **independently**, then take the **unweighted mean**. Treats all classes equally, regardless of frequency. | Classes are imbalanced, but you want **equal importance** given to each class |\n",
    "# | `'micro'`          | Aggregate total true positives, false negatives, and false positives before calculating F1.                                          | You care about **overall accuracy**, not per-class performance                |\n",
    "# | `'weighted'`       | Compute F1 per class, then take the **weighted mean by support (number of true instances)**.                                         | When classes are imbalanced, but you want to **weight by frequency**          |\n",
    "# | `None` or `'none'` | Returns F1 score **per class** (as an array)                                                                                         | You want class-specific insights                                              |\n",
    "\n",
    "\n",
    "f1_score_cal = f1_score(test_labels, predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27c2f69d-499f-4c0a-9ffb-202a2478d0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626569327935868"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bc4fe67-39b1-4871-97c9-756fbff1719e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa8f86e9-bf55-487c-af39-fc86b59db2c2",
   "metadata": {},
   "source": [
    "### EfficientNet B0 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5ae51a-d595-44f3-84ed-9b048b362406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image Data Generator\n",
    "train_path = r'D:/AIML/fer2013/train'\n",
    "train_datagen_eff = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_efficient,\n",
    "    rotation_range = 15,\n",
    "    zoom_range = 0.15,\n",
    "    width_shift_range= 0.1,\n",
    "    height_shift_range= 0.1,\n",
    "    shear_range = 0.1,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = 'nearest',\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "\n",
    "train_generator_eff = train_datagen_eff.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 8,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "val_generator_eff = train_datagen_eff.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 8,\n",
    "    class_mode = 'categorical',\n",
    "    subset ='validation',\n",
    "    shuffle=False,\n",
    "    seed = 42\n",
    ")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae149cb3-2a7b-4fcb-b2cc-4f0e6e7057c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weight\n",
    "\n",
    "class_weights_arr = class_weight.compute_class_weight(\n",
    "    class_weight = 'balanced',\n",
    "    classes = np.unique(train_generator_eff.classes),\n",
    "    y = train_generator_eff.classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dcbcb63-aabb-45e8-a417-3410c3a390c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_eff = dict(enumerate(class_weights_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67219508-99e1-4044-b34c-f4360febf968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "base_model = EfficientNetB0(weights = 'imagenet', include_top = False, input_shape=(224,224,3))\n",
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef9eb45-0f8e-4cbf-8540-f9e86d72f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet_model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(7, activation='softmax', dtype='float32')\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ef3e95-d8da-4ae7-9912-c5db42be9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet_model.compile( loss='categorical_crossentropy', optimizer = Adam(learning_rate=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ce7961-caf4-4a4f-b3b2-a2c4f028f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop_eff = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "lr_schedule_eff = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781607e5-f887-4408-b80a-1402914e08ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2871/2871 [==============================] - 1205s 416ms/step - loss: 1.6456 - accuracy: 0.3585 - val_loss: 1.3926 - val_accuracy: 0.4654 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "2871/2871 [==============================] - 476s 166ms/step - loss: 1.3165 - accuracy: 0.4943 - val_loss: 1.1670 - val_accuracy: 0.5581 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "2871/2871 [==============================] - 529s 184ms/step - loss: 1.1833 - accuracy: 0.5454 - val_loss: 1.1044 - val_accuracy: 0.5877 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "2871/2871 [==============================] - 490s 171ms/step - loss: 1.0902 - accuracy: 0.5805 - val_loss: 1.0677 - val_accuracy: 0.6070 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "2871/2871 [==============================] - 512s 178ms/step - loss: 1.0164 - accuracy: 0.6030 - val_loss: 1.0398 - val_accuracy: 0.6137 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "2871/2871 [==============================] - 448s 156ms/step - loss: 0.9679 - accuracy: 0.6214 - val_loss: 1.0787 - val_accuracy: 0.6032 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "2871/2871 [==============================] - 436s 152ms/step - loss: 0.9091 - accuracy: 0.6428 - val_loss: 1.1915 - val_accuracy: 0.5915 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "2871/2871 [==============================] - 434s 151ms/step - loss: 0.8827 - accuracy: 0.6498 - val_loss: 1.0046 - val_accuracy: 0.6351 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "2871/2871 [==============================] - 434s 151ms/step - loss: 0.8236 - accuracy: 0.6695 - val_loss: 1.0018 - val_accuracy: 0.6471 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "2871/2871 [==============================] - 447s 156ms/step - loss: 0.7949 - accuracy: 0.6801 - val_loss: 0.9914 - val_accuracy: 0.6452 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "2871/2871 [==============================] - 439s 153ms/step - loss: 0.7655 - accuracy: 0.6908 - val_loss: 1.0048 - val_accuracy: 0.6443 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "2871/2871 [==============================] - 448s 156ms/step - loss: 0.7317 - accuracy: 0.7006 - val_loss: 0.9926 - val_accuracy: 0.6515 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "2871/2871 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.7160\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "2871/2871 [==============================] - 440s 153ms/step - loss: 0.7001 - accuracy: 0.7160 - val_loss: 1.0218 - val_accuracy: 0.6539 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "2871/2871 [==============================] - 441s 154ms/step - loss: 0.6302 - accuracy: 0.7454 - val_loss: 1.0113 - val_accuracy: 0.6616 - lr: 5.0000e-05\n",
      "Epoch 15/30\n",
      "2871/2871 [==============================] - 442s 154ms/step - loss: 0.5991 - accuracy: 0.7569 - val_loss: 1.0620 - val_accuracy: 0.6626 - lr: 5.0000e-05\n",
      "Epoch 16/30\n",
      "2871/2871 [==============================] - ETA: 0s - loss: 0.5682 - accuracy: 0.7652\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "2871/2871 [==============================] - 465s 162ms/step - loss: 0.5682 - accuracy: 0.7652 - val_loss: 1.0388 - val_accuracy: 0.6609 - lr: 5.0000e-05\n",
      "Epoch 17/30\n",
      "2871/2871 [==============================] - 461s 160ms/step - loss: 0.5199 - accuracy: 0.7869 - val_loss: 1.0448 - val_accuracy: 0.6772 - lr: 2.5000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x203f4a02100>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnet_model.fit(\n",
    "    x = train_generator_eff,\n",
    "    validation_data = val_generator_eff,\n",
    "    epochs = 30,\n",
    "    class_weight = class_weights_eff,\n",
    "    callbacks = [early_stop_eff, lr_schedule_eff]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "866477e9-8208-48b9-a191-78d3fde37142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2871/2871 [==============================] - 385s 134ms/step - loss: 0.7913 - accuracy: 0.6809 - val_loss: 0.9611 - val_accuracy: 0.6549 - lr: 2.5000e-05\n",
      "Epoch 2/30\n",
      "2871/2871 [==============================] - 394s 137ms/step - loss: 0.7486 - accuracy: 0.6930 - val_loss: 0.9548 - val_accuracy: 0.6626 - lr: 2.5000e-05\n",
      "Epoch 3/30\n",
      "2871/2871 [==============================] - 388s 135ms/step - loss: 0.7314 - accuracy: 0.7029 - val_loss: 0.9561 - val_accuracy: 0.6603 - lr: 2.5000e-05\n",
      "Epoch 4/30\n",
      "2871/2871 [==============================] - 393s 137ms/step - loss: 0.7111 - accuracy: 0.7092 - val_loss: 0.9547 - val_accuracy: 0.6659 - lr: 2.5000e-05\n",
      "Epoch 5/30\n",
      "2871/2871 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.7192\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "2871/2871 [==============================] - 393s 137ms/step - loss: 0.6911 - accuracy: 0.7192 - val_loss: 0.9751 - val_accuracy: 0.6623 - lr: 2.5000e-05\n",
      "Epoch 6/30\n",
      "2871/2871 [==============================] - 395s 138ms/step - loss: 0.6705 - accuracy: 0.7289 - val_loss: 0.9560 - val_accuracy: 0.6741 - lr: 1.2500e-05\n",
      "Epoch 7/30\n",
      "2871/2871 [==============================] - 822s 286ms/step - loss: 0.6544 - accuracy: 0.7322 - val_loss: 0.9701 - val_accuracy: 0.6624 - lr: 1.2500e-05\n",
      "Epoch 8/30\n",
      "2871/2871 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.7361\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "2871/2871 [==============================] - 415s 145ms/step - loss: 0.6392 - accuracy: 0.7361 - val_loss: 0.9789 - val_accuracy: 0.6621 - lr: 1.2500e-05\n",
      "Epoch 9/30\n",
      "2871/2871 [==============================] - 348s 121ms/step - loss: 0.6318 - accuracy: 0.7395 - val_loss: 0.9663 - val_accuracy: 0.6744 - lr: 6.2500e-06\n",
      "Epoch 10/30\n",
      "2871/2871 [==============================] - 348s 121ms/step - loss: 0.6203 - accuracy: 0.7439 - val_loss: 0.9857 - val_accuracy: 0.6704 - lr: 6.2500e-06\n",
      "Epoch 11/30\n",
      "2871/2871 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.7460\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "2871/2871 [==============================] - 348s 121ms/step - loss: 0.6222 - accuracy: 0.7460 - val_loss: 0.9688 - val_accuracy: 0.6764 - lr: 6.2500e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28f27762520>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnet_model.fit(\n",
    "    x = train_generator_eff,\n",
    "    validation_data = val_generator_eff,\n",
    "    epochs = 30,\n",
    "    class_weight = class_weights_eff,\n",
    "    callbacks = [early_stop_eff, lr_schedule_eff]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a49b95-39d6-4d5e-9eda-7a526df9c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#effnet_model.save('effnet_model_saved.keras')  # Keras native format\n",
    "effnet_model.save_weights(\"effnet_model_saved.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1843270c-fbb3-462e-b767-584b65902006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check afterwards\n",
    "\n",
    "# Rebuild same architecture\n",
    "base_model_loaded = EfficientNetB0(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "base_model_loaded.trainable = True  # Or use .trainable = False if you want to freeze\n",
    "\n",
    "effnet_model_loaded = Sequential([\n",
    "    base_model_loaded,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(7, activation='softmax', dtype='float32')\n",
    "])\n",
    "\n",
    "# Compile the model again\n",
    "effnet_model_loaded.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "effnet_model_loaded.load_weights(\"effnet_model_saved.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba81ca-e389-47f7-8939-fcd2b7d13dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73c7b9a-69f2-48ad-ac5c-38ae2ab0502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#testing \n",
    "\n",
    "test_path = r'D:/AIML/fer2013/test'\n",
    "test_batches_eff = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_efficient).flow_from_directory(\n",
    "    directory = test_path,\n",
    "    target_size=(224,224),\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39cb5fb5-dcb1-4443-8a69-962db77d1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_eff = test_batches_eff.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d926b209-33e9-4dc0-8b1d-bf3f536fa287",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_eff = effnet_model_loaded.predict(x=test_batches_eff, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d27891-6b6e-4c49-8657-8d01c7d6bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_eff = np.argmax(predictions_eff, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba8c762-320f-4fc7-8b75-c8f5afad88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_cal_eff = f1_score(test_labels_eff, predicted_labels_eff, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5247fa45-6caf-48c8-a5b3-dd157e0e3966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6537826501281392"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_cal_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c0f86-235b-4164-af28-28b63d0e9796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23fca867-f78e-48ab-b50f-5053710135d5",
   "metadata": {},
   "source": [
    "### Ensemble Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7903cba-de48-4b88-bfc8-f05c80be3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure matching Order\n",
    "\n",
    "assert np.array_equal( test_batches_eff.filenames, test_batches.filenames), \"Image Order Mismatch!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0f85bd5-a98f-41e3-877e-184a249394d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Predictions\n",
    "\n",
    "ensemble_preds = (predictions_eff + predictions)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf49b35f-1c50-4fed-8928-9935854c0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_labels = np.argmax(ensemble_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0b5212f-6aeb-42ae-a80b-17ff82ca5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and F1-score\n",
    "acc_mobile = accuracy_score( test_labels_eff, predicted_labels)\n",
    "acc_eff = accuracy_score( test_labels_eff, predicted_labels_eff)\n",
    "acc_ensemble = accuracy_score( test_labels_eff, ensemble_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dfd454e-618e-4f42-a8c0-fe9a6dbeb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_mobile = f1_score( test_labels_eff, predicted_labels, average='macro')\n",
    "f1_effnet = f1_score ( test_labels_eff, predicted_labels_eff, average='macro')\n",
    "f1_ensemble = f1_score( test_labels_eff, ensemble_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b762ef5-a251-4008-b81e-e174d96d2b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy...........\n",
      "\n",
      "Mobile Net Accuracy: 66.72%\n",
      "Efficient Net Accuracy: 65.71%\n",
      "Ensemble Accuracy: 68.04%\n",
      "\n",
      "Macro F1 Score............\n",
      "\n",
      "Mobile Net : 0.6361\n",
      "Efficient Net: 0.6339\n",
      "Ensemble: 0.6552\n"
     ]
    }
   ],
   "source": [
    "# Print Results\n",
    "\n",
    "print('Testing Accuracy...........\\n')\n",
    "print( f\"Mobile Net Accuracy: {acc_mobile * 100:.2f}%\")\n",
    "print( f\"Efficient Net Accuracy: {acc_eff * 100:.2f}%\")\n",
    "print( f\"Ensemble Accuracy: {acc_ensemble * 100:.2f}%\")\n",
    "\n",
    "print('\\nMacro F1 Score............\\n')\n",
    "print(f\"Mobile Net : {f1_mobile:.4f}\")\n",
    "print(f\"Efficient Net: {f1_effnet:.4f}\")\n",
    "print(f\"Ensemble: {f1_ensemble:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5824b3-cef3-4928-bdca-791c7d261471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a097f2-3840-4100-84f9-e8fd3df2d434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1076f576-4a83-43f1-ad44-437de16955cd",
   "metadata": {},
   "source": [
    "### Ensemble Live Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cc3dfe-bab9-4a77-97ec-12f7c2824d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion classes\n",
    "emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fb8311-3b8a-4ddd-9e92-dca3bb646090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the models\n",
    "# Mobile Net\n",
    "mobile_model_load = load_model('mobile_model_saved.keras')\n",
    "\n",
    "# Efficient Net\n",
    "\n",
    "# Rebuild same architecture\n",
    "base_model_loaded = EfficientNetB0(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "base_model_loaded.trainable = True  # Or use .trainable = False if you want to freeze\n",
    "\n",
    "effnet_model_loaded = Sequential([\n",
    "    base_model_loaded,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(7, activation='softmax', dtype='float32')\n",
    "])\n",
    "\n",
    "# Compile the model again\n",
    "effnet_model_loaded.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "effnet_model_loaded.load_weights(\"effnet_model_saved.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2574344-eb90-4ca5-8c73-b2a4ab754a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Webcam.....press 'q' to quit.\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Start Webcam\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Haar Cascade for face detection\n",
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Starting Webcam.....press 'q' to quit.\")\n",
    "\n",
    "pTime = 0\n",
    "overall_emotion = {'Angry':0, 'Disgust':0, 'Fear':0, 'Happy':0, 'Neutral':1, 'Sad':0, 'Surprise':0}\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Frame capture failed\")\n",
    "        break\n",
    "\n",
    "\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect Face\n",
    "    faces = face_cascade.detectMultiScale( gray, scaleFactor=1.3, minNeighbors = 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Extract face Region-of-Interest(ROI) and convert to float32\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        face_img_resized = cv.resize(face_img, (224,224)).astype('float32')\n",
    "\n",
    "        face_batch_mobile = np.expand_dims(face_img_resized.copy(), axis = 0)\n",
    "        face_batch_effnet = np.expand_dims(face_img_resized.copy(), axis = 0)\n",
    "\n",
    "        # apply model specific preprocessing\n",
    "        face_batch_mobilenet = preprocess_mobile(face_batch_mobile)\n",
    "        face_batch_effnet = preprocess_efficient(face_batch_effnet)\n",
    "\n",
    "        #predict using both models\n",
    "        preds_mobile = mobile_model_load.predict(face_batch_mobilenet, verbose=0)\n",
    "        preds_effnet = effnet_model_loaded.predict(face_batch_effnet, verbose=0)\n",
    "\n",
    "        #Ensemble Average\n",
    "        ensemble_pred = (preds_mobile + preds_effnet)/2.0\n",
    "        print(np.argmax(ensemble_pred))\n",
    "        emotion = emotion_labels[np.argmax(ensemble_pred)]\n",
    "\n",
    "        #emotion = emotion_labels[np.argmax(preds_mobile)]\n",
    "\n",
    "        overall_emotion[emotion] +=1\n",
    "        most_common_emotion = max(overall_emotion, key = overall_emotion.get)\n",
    "       # emotion = emotion_labels[np.argmax(preds_effnet)]\n",
    "\n",
    "        cTime = time.time()\n",
    "        fps = 1/(cTime - pTime)\n",
    "        pTime = cTime\n",
    "\n",
    "        #Draw Box and display emotion\n",
    "        cv.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        cv.putText(frame, emotion, (x, y-10), cv.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "        cv.putText(frame, f'FPS: {int(fps)}', (x, y+h+20), cv.FONT_HERSHEY_PLAIN, 0.9, (36,255,12), 2)\n",
    "        cv.putText(frame, f'Overall Emotion: {most_common_emotion}', (x+100, y+h+20), cv.FONT_HERSHEY_PLAIN, 0.9, (36,255,12), 2)\n",
    "\n",
    "    cv.imshow('RealTime Emotion Detection', frame)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1366ac2b-e3c1-43b7-bdb9-03b12106b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Webcam.....press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Start Webcam\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Haar Cascade for face detection\n",
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Starting Webcam.....press 'q' to quit.\")\n",
    "\n",
    "pTime = 0\n",
    "overall_emotion = {'Angry':0, 'Disgust':0, 'Fear':0, 'Happy':0, 'Neutral':1, 'Sad':0, 'Surprise':0}\n",
    "frame_count = 0\n",
    "inference_interval = 5\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Frame capture failed\")\n",
    "        break\n",
    "\n",
    "    if frame_count % inference_interval == 0:\n",
    "        \n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "        # Detect Face\n",
    "        faces = face_cascade.detectMultiScale( gray, scaleFactor=1.3, minNeighbors = 5)\n",
    "    \n",
    "        for (x,y,w,h) in faces:\n",
    "            # Extract face Region-of-Interest(ROI) and convert to float32\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img_resized = cv.resize(face_img, (224,224)).astype('float32')\n",
    "    \n",
    "            face_batch_mobile = np.expand_dims(face_img_resized.copy(), axis = 0)\n",
    "            face_batch_effnet = np.expand_dims(face_img_resized.copy(), axis = 0)\n",
    "    \n",
    "            # apply model specific preprocessing\n",
    "            face_batch_mobilenet = preprocess_mobile(face_batch_mobile)\n",
    "            face_batch_effnet = preprocess_efficient(face_batch_effnet)\n",
    "    \n",
    "            #predict using both models\n",
    "            preds_mobile = mobile_model_load.predict(face_batch_mobilenet, verbose=0)\n",
    "            preds_effnet = effnet_model_loaded.predict(face_batch_effnet, verbose=0)\n",
    "    \n",
    "            #Ensemble Average\n",
    "            ensemble_pred = (preds_mobile + preds_effnet)/2.0\n",
    "            emotion = emotion_labels[np.argmax(ensemble_pred)]\n",
    "    \n",
    "            #emotion = emotion_labels[np.argmax(preds_mobile)]\n",
    "    \n",
    "            overall_emotion[emotion] +=1\n",
    "            most_common_emotion = max(overall_emotion, key = overall_emotion.get)\n",
    "           # emotion = emotion_labels[np.argmax(preds_effnet)]\n",
    "    \n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime - pTime)\n",
    "    pTime = cTime\n",
    "    \n",
    "    #Draw Box and display emotion\n",
    "    cv.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "    cv.putText(frame, emotion, (x, y-10), cv.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "    cv.putText(frame, f'FPS: {int(fps)}', (x, y+h+20), cv.FONT_HERSHEY_PLAIN, 0.9, (36,255,12), 2)\n",
    "    cv.putText(frame, f'Overall Emotion: {most_common_emotion}', (x+100, y+h+20), cv.FONT_HERSHEY_PLAIN, 0.9, (36,255,12), 2)\n",
    "\n",
    "\n",
    "    frame_count += 1\n",
    "    cv.imshow('RealTime Emotion Detection', frame)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46b92a-8c6e-4f46-84d9-2c04d9702426",
   "metadata": {},
   "source": [
    "### Final One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681332ba-8af2-4edb-8c35-6cc6ae9484b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Webcam.....press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# Start Webcam\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Haar Cascade for face detection\n",
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Starting Webcam.....press 'q' to quit.\")\n",
    "\n",
    "pTime = 0\n",
    "overall_emotion = {label: 0 for label in ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']}\n",
    "frame_count = 0\n",
    "inference_interval = 5\n",
    "\n",
    "# To retain previous prediction\n",
    "last_emotion = \"Detecting...\"\n",
    "last_box = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Frame capture failed\")\n",
    "        break\n",
    "\n",
    "    # Always calculate FPS\n",
    "    cTime = time.time()\n",
    "    fps = 1 / (cTime - pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    if frame_count % inference_interval == 0:\n",
    "        # Convert to grayscale and equalize histogram\n",
    "        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        gray = cv.equalizeHist(gray)\n",
    "\n",
    "        # Detect Face\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]  # take only the first face\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img_resized = cv.resize(face_img, (224, 224)).astype('float32')\n",
    "\n",
    "            face_batch_mobile = np.expand_dims(face_img_resized.copy(), axis=0)\n",
    "            face_batch_effnet = np.expand_dims(face_img_resized.copy(), axis=0)\n",
    "\n",
    "            # Preprocessing\n",
    "            face_batch_mobilenet = preprocess_mobile(face_batch_mobile)\n",
    "            face_batch_effnet = preprocess_efficient(face_batch_effnet)\n",
    "\n",
    "            # Predict\n",
    "            preds_mobile = mobile_model_load.predict(face_batch_mobilenet, verbose=0)\n",
    "            preds_effnet = effnet_model_loaded.predict(face_batch_effnet, verbose=0)\n",
    "\n",
    "            # Ensemble\n",
    "            ensemble_pred = (preds_mobile + preds_effnet) / 2.0\n",
    "            emotion = emotion_labels[np.argmax(ensemble_pred)]\n",
    "\n",
    "            # Save for next frames\n",
    "            last_emotion = emotion\n",
    "            last_box = (x, y, w, h)\n",
    "\n",
    "            overall_emotion[emotion] += 1\n",
    "\n",
    "    most_common_emotion = max(overall_emotion, key=overall_emotion.get)\n",
    "\n",
    "    # Draw previous prediction if available\n",
    "    if last_box is not None:\n",
    "        x, y, w, h = last_box\n",
    "        cv.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv.putText(frame, last_emotion, (x, y-10), cv.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "        cv.putText(frame, f'Overall: {most_common_emotion}', (x+100, y+h+20), cv.FONT_HERSHEY_PLAIN, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    # Always show FPS\n",
    "    cv.putText(frame, f'FPS: {int(fps)}', (10, 30), cv.FONT_HERSHEY_PLAIN, 1, (36, 255, 12), 2)\n",
    "\n",
    "    frame_count += 1\n",
    "    cv.imshow('RealTime Emotion Detection', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3666abd-7fa3-4b02-bef3-94cdbb2c27ff",
   "metadata": {},
   "source": [
    "### Increased FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d04ce7-26b4-427b-8951-4f5de0248426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Webcam.....press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# Initialize video capture\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Haar cascade for face detection\n",
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Starting Webcam.....press 'q' to quit.\")\n",
    "\n",
    "pTime = 0\n",
    "frame_count = 0\n",
    "inference_interval = 5\n",
    "\n",
    "# Emotion counters\n",
    "overall_emotion = {'Angry': 0, 'Disgust': 0, 'Fear': 0, 'Happy': 0, 'Neutral': 1, 'Sad': 0, 'Surprise': 0}\n",
    "most_common_emotion = 'Neutral'\n",
    "\n",
    "# Track last detections\n",
    "last_detections = []  # List of (x, y, w, h, emotion)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Frame capture failed\")\n",
    "        break\n",
    "\n",
    "    if frame_count % inference_interval == 0:\n",
    "        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            last_detections = []\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "                face_img_resized = cv.resize(face_img, (224, 224)).astype('float32')\n",
    "\n",
    "                face_batch_mobile = np.expand_dims(face_img_resized.copy(), axis=0)\n",
    "                face_batch_effnet = np.expand_dims(face_img_resized.copy(), axis=0)\n",
    "\n",
    "                # Apply model-specific preprocessing\n",
    "                face_batch_mobilenet = preprocess_mobile(face_batch_mobile)\n",
    "                face_batch_effnet = preprocess_efficient(face_batch_effnet)\n",
    "\n",
    "                # Get predictions from both models\n",
    "                preds_mobile = mobile_model_load.predict(face_batch_mobilenet, verbose=0)\n",
    "                preds_effnet = effnet_model_loaded.predict(face_batch_effnet, verbose=0)\n",
    "\n",
    "                # Ensemble average\n",
    "                ensemble_pred = (preds_mobile + preds_effnet) / 2.0\n",
    "                emotion = emotion_labels[np.argmax(ensemble_pred)]\n",
    "\n",
    "                # Update emotion tracking\n",
    "                overall_emotion[emotion] += 1\n",
    "                most_common_emotion = max(overall_emotion, key=overall_emotion.get)\n",
    "\n",
    "                last_detections.append((x, y, w, h, emotion))\n",
    "\n",
    "    # Draw last known detections on every frame\n",
    "    for (x, y, w, h, emotion) in last_detections:\n",
    "        cv.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv.putText(frame, emotion, (x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "        cv.putText(frame, f'Overall Emotion: {most_common_emotion}', (x + 100, y + h + 20),\n",
    "                   cv.FONT_HERSHEY_PLAIN, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    # FPS calculation\n",
    "    cTime = time.time()\n",
    "    fps = 1 / (cTime - pTime) if cTime != pTime else 0\n",
    "    pTime = cTime\n",
    "    cv.putText(frame, f'FPS: {int(fps)}', (20, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv.imshow('RealTime Emotion Detection', frame)\n",
    "    frame_count += 1\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6e683-5f6f-4b8b-83bd-2ea95a2df5a5",
   "metadata": {},
   "source": [
    "#### A. Temporal Emotion Smoothing\n",
    "Keeps a history of the last N=10 predicted emotions.\n",
    "\n",
    "Displays the most common emotion to reduce jitter.\n",
    "\n",
    "#### B. Frame Downscaling for Faster Face Detection\n",
    "Speeds up face detection significantly with no major quality loss.\n",
    "\n",
    "#### C. Threading for Non-Blocking Emotion Inference\n",
    "Keeps UI smooth while predictions run in a background thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231e1341-2376-418a-8a18-c07b6fd686a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Webcam.....press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "from collections import deque, Counter\n",
    "from queue import Queue\n",
    "\n",
    "# Your model loading & preprocessing code here\n",
    "# Example:\n",
    "# mobile_model_load = ...\n",
    "# effnet_model_loaded = ...\n",
    "# preprocess_mobile = ...\n",
    "# preprocess_efficient = ...\n",
    "# emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv.VideoCapture(0)\n",
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Starting Webcam.....press 'q' to quit.\")\n",
    "\n",
    "# Emotion history\n",
    "emotion_history = deque(maxlen=10)\n",
    "overall_emotion = {label: 0 for label in ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']}\n",
    "overall_emotion['Neutral'] = 1  # Start with some value\n",
    "most_common_emotion = 'Neutral'\n",
    "\n",
    "# Threading result queue\n",
    "results_queue = Queue()\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Store last predictions\n",
    "last_detections = []\n",
    "\n",
    "def process_faces(frame, faces):\n",
    "    global last_detections, overall_emotion, most_common_emotion\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = frame[y:y + h, x:x + w]\n",
    "        face_img_resized = cv.resize(face_img, (224, 224)).astype('float32')\n",
    "\n",
    "        face_batch_mobile = np.expand_dims(face_img_resized.copy(), axis=0)\n",
    "        face_batch_effnet = np.expand_dims(face_img_resized.copy(), axis=0)\n",
    "\n",
    "        face_batch_mobilenet = preprocess_mobile(face_batch_mobile)\n",
    "        face_batch_effnet = preprocess_efficient(face_batch_effnet)\n",
    "\n",
    "        preds_mobile = mobile_model_load.predict(face_batch_mobilenet, verbose=0)\n",
    "        preds_effnet = effnet_model_loaded.predict(face_batch_effnet, verbose=0)\n",
    "\n",
    "        ensemble_pred = (preds_mobile + preds_effnet) / 2.0\n",
    "        raw_emotion = emotion_labels[np.argmax(ensemble_pred)]\n",
    "\n",
    "        # Add to history for smoothing\n",
    "        emotion_history.append(raw_emotion)\n",
    "        smoothed_emotion = Counter(emotion_history).most_common(1)[0][0]\n",
    "\n",
    "        with lock:\n",
    "            overall_emotion[smoothed_emotion] += 1\n",
    "            most_common_emotion = max(overall_emotion, key=overall_emotion.get)\n",
    "\n",
    "        detections.append((x, y, w, h, smoothed_emotion))\n",
    "\n",
    "    results_queue.put(detections)\n",
    "\n",
    "pTime = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Frame capture failed\")\n",
    "        break\n",
    "\n",
    "    # Resize for faster detection\n",
    "    scale = 0.5\n",
    "    small_frame = cv.resize(frame, (0, 0), fx=scale, fy=scale)\n",
    "    gray_small = cv.cvtColor(small_frame, cv.COLOR_BGR2GRAY)\n",
    "    faces_small = face_cascade.detectMultiScale(gray_small, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # Scale boxes back to original size\n",
    "    faces = [(int(x / scale), int(y / scale), int(w / scale), int(h / scale)) for (x, y, w, h) in faces_small]\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        thread = threading.Thread(target=process_faces, args=(frame.copy(), faces))\n",
    "        thread.start()\n",
    "\n",
    "    # Update detection results if thread has finished\n",
    "    if not results_queue.empty():\n",
    "        last_detections = results_queue.get()\n",
    "\n",
    "    # Draw results from last detection\n",
    "    for (x, y, w, h, emotion) in last_detections:\n",
    "        cv.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv.putText(frame, emotion, (x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "        cv.putText(frame, f'Overall: {most_common_emotion}', (x + 100, y + h + 20),\n",
    "                   cv.FONT_HERSHEY_PLAIN, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    # FPS calculation\n",
    "    cTime = time.time()\n",
    "    fps = 1 / (cTime - pTime) if cTime != pTime else 0\n",
    "    pTime = cTime\n",
    "    cv.putText(frame, f'FPS: {int(fps)}', (20, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    cv.imshow('RealTime Emotion Detection', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ced745-6ca5-4521-8c09-fd14834f4c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f28ba-1e8d-4917-a81e-435b185ee03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e1fc9-410a-4873-9deb-02353aa2ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0421d07-7fff-401c-815b-c4208575b686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac91e33-6d16-46e6-9be8-2e48371789d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4414866-5bdb-48e9-95b2-33fd0f8d8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "def plot_accuracy_loss(history, save_path='accuracy_loss.png'):\n",
    "    acc = history.history.get('accuracy', [])\n",
    "    val_acc = history.history.get('val_accuracy', [])\n",
    "    loss = history.history.get('loss', [])\n",
    "    val_loss = history.history.get('val_loss', [])\n",
    "    epochs = range(1, len(acc)+1)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, label='Train Acc')\n",
    "    plt.plot(epochs, val_acc, label='Val Acc')\n",
    "    plt.title('Accuracy vs Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Val Loss')\n",
    "    plt.title('Loss vs Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion(y_true, y_pred, labels, save_path='confusion_matrix.png'):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_fps(before_fps, after_fps, save_path='fps_comparison.png'):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.bar(['Before', 'After'], [before_fps, after_fps], color=['red', 'green'])\n",
    "    plt.ylabel('FPS')\n",
    "    plt.title('FPS Improvement')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_emotion_distribution(emotion_counts, save_path='emotion_distribution.png'):\n",
    "    \"\"\"\n",
    "    emotion_counts: dictionary like {'happy': 5000, 'sad': 4000, ...}\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(list(emotion_counts.items()), columns=['Emotion', 'Count'])\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(data=df, x='Emotion', y='Count', palette='pastel')\n",
    "    plt.title('Emotion Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68638be3-1148-47b1-b585-1dc4fa5ddb37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
